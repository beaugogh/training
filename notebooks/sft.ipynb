{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde2b4a1-3c6c-4a7d-a7ec-dc35f959689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"info\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from model import HuggingfaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89da7cf8-6953-46d3-b7ff-94521b5116a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
    "dataset = dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c27521-a637-43ca-93c5-7a52a2157abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cast_datast_to_instruction_format\n",
    "dataset = cast_datast_to_instruction_format(dataset, \"text\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1774e49a-8a37-4388-9649-48df935f2f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90755278-c6d5-4597-983a-c119d283aa2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"I can't believe that those praising this movie herein aren't thinking of some other film. I was prepared for the possibility that this would be awful, but the script (or lack thereof) makes for a film that's also pointless. On the plus side, the general level of craft on the part of the actors and technical crew is quite competent, but when you've got a sow's ear to work with you can't make a silk purse. Ben G fans should stick with just about any other movie he's been in. Dorothy S fans should stick to Galaxina. Peter B fans should stick to Last Picture Show and Target. Fans of cheap laughs at the expense of those who seem to be asking for it should stick to Peter B's amazingly awful book, Killing of the Unicorn.\",\n",
       " 'completion': '0'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd416d32-6416-4c3f-8ebc-1a829c869de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/bo/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/e6de91484c29aa9480d55605af694f39b081c455/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/bo/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/e6de91484c29aa9480d55605af694f39b081c455/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-0.6B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/bo/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/e6de91484c29aa9480d55605af694f39b081c455/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9070dc6e-131b-4f12-ba33-7e6d7bf71ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file vocab.json from cache at /home/bo/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/e6de91484c29aa9480d55605af694f39b081c455/vocab.json\n",
      "loading file merges.txt from cache at /home/bo/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/e6de91484c29aa9480d55605af694f39b081c455/merges.txt\n",
      "loading file tokenizer.json from cache at /home/bo/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/e6de91484c29aa9480d55605af694f39b081c455/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/bo/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/e6de91484c29aa9480d55605af694f39b081c455/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53eb6c6525874fb5bd0329acd4da824d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce09df2ba44a4c4e85a152aedab88756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae2f471b8c84adc95c54ef24432d559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f455cd54bd4540a74518c9739a95a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-14 07:48:13,342] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-06-14 07:48:15,602] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the Training set don't have a corresponding argument in `Qwen3ForCausalLM.forward` and have been ignored: completion, prompt. If completion, prompt are not expected by `Qwen3ForCausalLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 125\n",
      "  Number of trainable parameters = 596,049,920\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 25:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to qwen3-0.6B-imdb/checkpoint-30\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-30/config.json\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-30/generation_config.json\n",
      "Model weights saved in qwen3-0.6B-imdb/checkpoint-30/model.safetensors\n",
      "chat template saved in qwen3-0.6B-imdb/checkpoint-30/chat_template.jinja\n",
      "tokenizer config file saved in qwen3-0.6B-imdb/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in qwen3-0.6B-imdb/checkpoint-30/special_tokens_map.json\n",
      "Saving model checkpoint to qwen3-0.6B-imdb/checkpoint-60\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-60/config.json\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-60/generation_config.json\n",
      "Model weights saved in qwen3-0.6B-imdb/checkpoint-60/model.safetensors\n",
      "chat template saved in qwen3-0.6B-imdb/checkpoint-60/chat_template.jinja\n",
      "tokenizer config file saved in qwen3-0.6B-imdb/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in qwen3-0.6B-imdb/checkpoint-60/special_tokens_map.json\n",
      "Saving model checkpoint to qwen3-0.6B-imdb/checkpoint-90\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-90/config.json\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-90/generation_config.json\n",
      "Model weights saved in qwen3-0.6B-imdb/checkpoint-90/model.safetensors\n",
      "chat template saved in qwen3-0.6B-imdb/checkpoint-90/chat_template.jinja\n",
      "tokenizer config file saved in qwen3-0.6B-imdb/checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in qwen3-0.6B-imdb/checkpoint-90/special_tokens_map.json\n",
      "Saving model checkpoint to qwen3-0.6B-imdb/checkpoint-120\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-120/config.json\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-120/generation_config.json\n",
      "Model weights saved in qwen3-0.6B-imdb/checkpoint-120/model.safetensors\n",
      "chat template saved in qwen3-0.6B-imdb/checkpoint-120/chat_template.jinja\n",
      "tokenizer config file saved in qwen3-0.6B-imdb/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in qwen3-0.6B-imdb/checkpoint-120/special_tokens_map.json\n",
      "Saving model checkpoint to qwen3-0.6B-imdb/checkpoint-125\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-125/config.json\n",
      "Configuration saved in qwen3-0.6B-imdb/checkpoint-125/generation_config.json\n",
      "Model weights saved in qwen3-0.6B-imdb/checkpoint-125/model.safetensors\n",
      "chat template saved in qwen3-0.6B-imdb/checkpoint-125/chat_template.jinja\n",
      "tokenizer config file saved in qwen3-0.6B-imdb/checkpoint-125/tokenizer_config.json\n",
      "Special tokens file saved in qwen3-0.6B-imdb/checkpoint-125/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.19899940490722656, metrics={'train_runtime': 1527.4019, 'train_samples_per_second': 0.655, 'train_steps_per_second': 0.082, 'total_flos': 1244401609211904.0, 'train_loss': 0.19899940490722656})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = SFTConfig(\n",
    "    # learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=30,\n",
    "    max_length=512, \n",
    "    # per_device_train_batch_size=6,\n",
    "    logging_steps=10,\n",
    "    report_to=\"tensorboard\", \n",
    "    eos_token=\"<|im_end|>\",\n",
    "    output_dir=\"qwen3-0.6B-imdb\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd51ee93-f02e-44ad-8043-5502c69174ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file /home/bo/workspace/training/qwen3-0.6B-imdb/checkpoint-120/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file /home/bo/workspace/training/qwen3-0.6B-imdb/checkpoint-120/model.safetensors\n",
      "Will use torch_dtype=torch.float32 as defined in model's config object\n",
      "Instantiating Qwen3ForCausalLM model under default dtype torch.float32.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at /home/bo/workspace/training/qwen3-0.6B-imdb/checkpoint-120.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
      "loading configuration file /home/bo/workspace/training/qwen3-0.6B-imdb/checkpoint-120/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model /home/bo/workspace/training/qwen3-0.6B-imdb/checkpoint-120 loaded successfully. Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "hfmodel = HuggingfaceModel(\"/home/bo/workspace/training/qwen3-0.6B-imdb/checkpoint-120\")\n",
    "# model_name = \"models/Qwen3-4B\"\n",
    "# model_name = \"Qwen/Qwen3-0.6B\"\n",
    "# model_name = \"Qwen/Qwen3-0.6B-base\"\n",
    "# model_name = \"opt-350m-imdb/checkpoint-5500\"\n",
    "# model = HuggingfaceModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ff6d65b-8b8a-4c33-8094-1bcff42f1db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hfmodel.generate(\"what about it\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfbbb088-2aa0-456b-9555-c7b2362db3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = hfmodel.tokenizer([\"this is great\"], return_tensors=\"pt\").to(\n",
    "    hfmodel.model.device\n",
    ")\n",
    "model_outputs = hfmodel.model.generate(\n",
    "    **model_inputs, max_new_tokens=512, temperature=0.1\n",
    ")\n",
    "output_ids = model_outputs[0][len(model_inputs.input_ids[0]) :].tolist()\n",
    "response = hfmodel.tokenizer.decode(\n",
    "    output_ids, skip_special_tokens=True\n",
    ").strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6f1ba9b-7ce8-4305-bdd3-51f893a7ec42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8240df1-0d44-48d8-b5b2-a70d2984be5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
